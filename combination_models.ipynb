{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAVOaL60L5s2",
        "outputId": "337bbab3-7cd2-446c-c03f-ef4fc91a58a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying combination of augmentation functions to check roughly how it's applying on original tiles"
      ],
      "metadata": {
        "id": "IarWjXoOzu44"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpkE0FuOMH9p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import apply_brightness_shift, apply_channel_shift, random_channel_shift, random_brightness\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define Image Directory\n",
        "image_dir = \"/content/drive/MyDrive/test/class_a\"\n",
        "augmented_dir = \"/content/drive/MyDrive/augmented_images\"\n",
        "os.makedirs(augmented_dir, exist_ok=True)\n",
        "\n",
        "# Load Images and Preprocess\n",
        "def load_images(image_dir):\n",
        "    images = []\n",
        "    filenames = sorted(os.listdir(image_dir))\n",
        "\n",
        "    for filename in filenames:\n",
        "        img_path = os.path.join(image_dir, filename)\n",
        "\n",
        "        # Read the image\n",
        "        img = cv2.imread(img_path)\n",
        "\n",
        "        # Check if the image is loaded properly\n",
        "        if img is None:\n",
        "            print(f\"Skipping {filename}: Unable to load image.\")\n",
        "            continue\n",
        "\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (256, 256)) / 255.0\n",
        "        images.append((filename, img))\n",
        "\n",
        "    print(f\"Loaded {len(images)} images successfully.\")\n",
        "    return images\n",
        "\n",
        "\n",
        "image_data = load_images(image_dir)\n",
        "print(f\"Loaded {len(image_data)} images.\")\n",
        "\n",
        "# Augmentation Functions (Convert NumPy to Tensor)\n",
        "# Apply Brightness Shift\n",
        "def augment_brightness(image, delta=0.2):\n",
        "    return tf.image.adjust_brightness(image, delta).numpy() if isinstance(image, tf.Tensor) else tf.image.adjust_brightness(tf.convert_to_tensor(image), delta).numpy()\n",
        "\n",
        "# Apply Channel Shift\n",
        "def augment_channel_shift(image, intensity=10.0):\n",
        "    return apply_channel_shift(image, intensity) if isinstance(image, np.ndarray) else apply_channel_shift(image.numpy(), intensity)\n",
        "\n",
        "# Apply Random Channel Shift\n",
        "def augment_random_channel_shift(image, max_delta=0.2):\n",
        "    return random_channel_shift(image, max_delta) if isinstance(image, np.ndarray) else random_channel_shift(image.numpy(), max_delta)\n",
        "\n",
        "# Apply Random Brightness (Final Step)\n",
        "def augment_random_brightness(image, delta=0.3):\n",
        "    return tf.image.adjust_brightness(image, delta).numpy() if isinstance(image, tf.Tensor) else tf.image.adjust_brightness(tf.convert_to_tensor(image), delta).numpy()\n",
        "\n",
        "\n",
        "\n",
        "# Augmentation Combinations\n",
        "augmentation_combinations = {\n",
        "    \"brightness_shift\": augment_brightness,\n",
        "    \"channel_shift\": augment_channel_shift,\n",
        "    \"random_channel_shift\": augment_random_channel_shift,\n",
        "    \"random_brightness\": augment_random_brightness,\n",
        "    \"brightness_and_channel\": lambda img: augment_channel_shift(augment_brightness(img)),\n",
        "    \"brightness_and_random_channel\": lambda img: augment_random_channel_shift(augment_brightness(img)),\n",
        "    \"channel_and_random_brightness\": lambda img: augment_random_brightness(augment_channel_shift(img)),\n",
        "    \"all_combined\": lambda img: augment_random_brightness(augment_random_channel_shift(augment_brightness(img)))\n",
        "}\n",
        "\n",
        "# Apply Augmentations and Save Images\n",
        "for aug_name, aug_function in augmentation_combinations.items():\n",
        "    aug_path = os.path.join(augmented_dir, aug_name, \"class_a\")\n",
        "    os.makedirs(aug_path, exist_ok=True)\n",
        "\n",
        "    for filename, img in image_data:\n",
        "        augmented_img = aug_function(img)\n",
        "        augmented_img = np.clip(augmented_img, 0, 1)\n",
        "        save_path = os.path.join(aug_path, filename)\n",
        "        cv2.imwrite(save_path, cv2.cvtColor((augmented_img * 255).astype(np.uint8), cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    print(f\"Augmented images saved in: {aug_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying 3 augmentation technique processing for artifacts removal/minimizing methods of Random channel shift, Brightness shift and both Brightness with random channel"
      ],
      "metadata": {
        "id": "ldpE5DcnytTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.image import apply_channel_shift\n",
        "\n",
        "# Define Paths\n",
        "base_dir = \"/content/drive/MyDrive\"\n",
        "class_paths = {\n",
        "    \"train_class_a\": os.path.join(base_dir, \"train\", \"class_a\"),\n",
        "    \"train_class_b\": os.path.join(base_dir, \"train\", \"class_b\"),\n",
        "    \"val_class_a\": os.path.join(base_dir, \"val\", \"class_a\"),\n",
        "    \"val_class_b\": os.path.join(base_dir, \"val\", \"class_b\"),\n",
        "}\n",
        "\n",
        "# Augmentation Settings\n",
        "augment_factor = 3\n",
        "\n",
        "# Define Augmentation Functions\n",
        "def augment_brightness(image, delta=0.2):\n",
        "    \"\"\"Adjust brightness\"\"\"\n",
        "    return tf.image.adjust_brightness(image, delta).numpy()\n",
        "\n",
        "def augment_random_channel_shift(image, max_delta=0.2):\n",
        "    \"\"\"Apply random channel shift\"\"\"\n",
        "    return tf.image.random_saturation(image, 1 - max_delta, 1 + max_delta).numpy()\n",
        "\n",
        "def augment_brightness_random_channel(image):\n",
        "    \"\"\"Apply brightness and random channel shift together\"\"\"\n",
        "    img = augment_brightness(image)\n",
        "    return augment_random_channel_shift(img)\n",
        "\n",
        "# Augmentations Dictionary\n",
        "augmentations = {\n",
        "    \"random_channel_shift\": augment_random_channel_shift,\n",
        "   # \"brightness_shift\": augment_brightness,\n",
        "   # \"brightness_random_channel\": augment_brightness_random_channel\n",
        "}\n",
        "\n",
        "# Apply Augmentations\n",
        "for cls, cls_path in class_paths.items():\n",
        "    print(f\"Processing {cls} augmentation...\")\n",
        "\n",
        "    # Check if directory exists before processing\n",
        "    if not os.path.exists(cls_path):\n",
        "        print(f\"Skipping {cls}: Directory not found -> {cls_path}\")\n",
        "        continue\n",
        "\n",
        "    images = [img for img in os.listdir(cls_path) if img.endswith(\".png\")]\n",
        "\n",
        "    for aug_name, aug_function in augmentations.items():\n",
        "        augmented_dir = f\"{cls_path}_{aug_name}_augmented\"\n",
        "        os.makedirs(augmented_dir, exist_ok=True)\n",
        "\n",
        "        for img_name in tqdm(images, desc=f\"Augmenting {cls} with {aug_name}\"):\n",
        "            img_path = os.path.join(cls_path, img_name)\n",
        "            image = Image.open(img_path)\n",
        "            image = image.convert(\"RGB\")\n",
        "\n",
        "            # Convert image to NumPy array and normalize\n",
        "            image_np = np.array(image) / 255.0\n",
        "\n",
        "            # Save original image\n",
        "            image.save(os.path.join(augmented_dir, img_name))\n",
        "\n",
        "            # Generate augmentations\n",
        "            for i in range(augment_factor):\n",
        "                augmented_img = aug_function(image_np)\n",
        "                augmented_img = np.clip(augmented_img, 0, 1)\n",
        "                augmented_img = (augmented_img * 255).astype(np.uint8)\n",
        "\n",
        "                # Save augmented image\n",
        "                aug_img_name = f\"{os.path.splitext(img_name)[0]}_{aug_name}_aug_{i}.png\"\n",
        "                aug_img_path = os.path.join(augmented_dir, aug_img_name)\n",
        "                Image.fromarray(augmented_img).save(aug_img_path)\n",
        "\n",
        "        print(f\"Augmented images saved in: {augmented_dir}\")\n",
        "\n",
        "print(\"Data augmentation completed successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9YcBwp2NS7h",
        "outputId": "b6ceda6d-529a-4164-c71d-f33b70c9cf4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train_class_a augmentation...\n",
            "Skipping train_class_a: Directory not found -> /content/drive/MyDrive/train/class_a\n",
            "Processing train_class_b augmentation...\n",
            "Skipping train_class_b: Directory not found -> /content/drive/MyDrive/train/class_b\n",
            "Processing val_class_a augmentation...\n",
            "Skipping val_class_a: Directory not found -> /content/drive/MyDrive/val/class_a\n",
            "Processing val_class_b augmentation...\n",
            "Skipping val_class_b: Directory not found -> /content/drive/MyDrive/val/class_b\n",
            "✅ Data augmentation completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4AwoyRLxtFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientnetB0 model training for the augmented dataset of Channel shift method"
      ],
      "metadata": {
        "id": "ZYVfWlsNx8nj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BVsE2oxWLie4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, Normalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Disable Mixed Precision (use float32 everywhere)\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "set_global_policy('float32')\n",
        "\n",
        "# Copy dataset from Google Drive to Colab RAM (Faster than streaming)\n",
        "os.makedirs('/content/dataset', exist_ok=True)\n",
        "if not os.path.exists('/content/dataset/train'):\n",
        "    print(\"Copying dataset to Colab RAM...\")\n",
        "    shutil.copytree('/content/drive/MyDrive/train', '/content/dataset/train')\n",
        "    shutil.copytree('/content/drive/MyDrive/val', '/content/dataset/val')\n",
        "    shutil.copytree('/content/drive/MyDrive/test', '/content/dataset/test')\n",
        "\n",
        "# Define dataset directories\n",
        "train_dir = '/content/dataset/train'\n",
        "val_dir = '/content/dataset/val'\n",
        "test_dir = '/content/dataset/test'\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 128\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Load dataset using `tf.data`\n",
        "raw_train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "class_names = raw_train_dataset.class_names\n",
        "\n",
        "# Define Data Augmentation Layer\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "    tf.keras.layers.RandomBrightness(0.1),\n",
        "])\n",
        "\n",
        "# Define normalization layer\n",
        "normalization_layer = Normalization()\n",
        "\n",
        "# Function to convert images to float32\n",
        "def cast_to_float32(image, label):\n",
        "    return tf.cast(image, tf.float32), label\n",
        "\n",
        "# Function to apply augmentation + normalization\n",
        "def preprocess_image(image, label):\n",
        "    image = data_augmentation(image)\n",
        "    image = normalization_layer(image)\n",
        "    return image, label\n",
        "\n",
        "# Convert dataset images to float32 before applying normalization\n",
        "normalized_train_dataset = raw_train_dataset.map(cast_to_float32)\n",
        "\n",
        "# Adapt Normalization Layer using training data (only on images)\n",
        "normalization_layer.adapt(normalized_train_dataset.map(lambda x, y: x))\n",
        "\n",
        "# Apply augmentation & normalization to dataset\n",
        "train_dataset = normalized_train_dataset.map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ").map(cast_to_float32).map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ").map(cast_to_float32).map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Load Pre-trained EfficientNetB0 with Correct Input Shape\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add Custom Classification Layers\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu', kernel_regularizer=l2(0.005))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.6)(x)  # Dropout Increased\n",
        "x = Dense(256, activation='relu', kernel_regularizer=l2(0.005))(x)\n",
        "x = Dropout(0.6)(x)\n",
        "\n",
        "# Use `len(class_names)`\n",
        "outputs = Dense(len(class_names), activation='softmax')(x)\n",
        "\n",
        "# Define the Model\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# Freeze the First 120 Layers (EfficientNetB0 has ~236 layers)\n",
        "for layer in base_model.layers[:120]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile Model with Optimized Learning Rate\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=5e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Add Early Stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=15,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Unfreeze More Layers & Fine-tune\n",
        "for layer in base_model.layers[80:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# **Implement Learning Rate Decay**\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=5e-5,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.95\n",
        ")\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# Compile Again with Learning Rate Decay\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "history_fine = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate on Test Set\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "# Save Model\n",
        "model.save('/content/drive/MyDrive/saved_models/efficientnet_model3.keras')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eb593b1-24fa-4546-ed31-65163605cfda",
        "id": "TCTr2alALnH4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying dataset to Colab RAM...\n",
            "Found 16460 files belonging to 2 classes.\n",
            "Found 4668 files belonging to 2 classes.\n",
            "Found 615 files belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 2s/step - accuracy: 0.7223 - loss: 7.6838 - val_accuracy: 0.5315 - val_loss: 5.4089\n",
            "Epoch 2/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.8606 - loss: 3.5717 - val_accuracy: 0.8278 - val_loss: 2.1358\n",
            "Epoch 3/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9119 - loss: 1.6978 - val_accuracy: 0.9128 - val_loss: 1.1909\n",
            "Epoch 4/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9241 - loss: 1.0476 - val_accuracy: 0.9225 - val_loss: 0.8391\n",
            "Epoch 5/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9304 - loss: 0.7494 - val_accuracy: 0.9269 - val_loss: 0.6428\n",
            "Epoch 6/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9448 - loss: 0.5511 - val_accuracy: 0.9312 - val_loss: 0.5326\n",
            "Epoch 7/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9497 - loss: 0.4445 - val_accuracy: 0.9231 - val_loss: 0.4610\n",
            "Epoch 8/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9533 - loss: 0.3578 - val_accuracy: 0.9055 - val_loss: 0.4563\n",
            "Epoch 9/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9578 - loss: 0.2896 - val_accuracy: 0.9321 - val_loss: 0.3482\n",
            "Epoch 10/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9637 - loss: 0.2541 - val_accuracy: 0.9287 - val_loss: 0.3037\n",
            "Epoch 11/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9624 - loss: 0.2232 - val_accuracy: 0.9085 - val_loss: 0.3841\n",
            "Epoch 12/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9638 - loss: 0.2340 - val_accuracy: 0.9308 - val_loss: 0.3097\n",
            "Epoch 13/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9696 - loss: 0.1921 - val_accuracy: 0.9379 - val_loss: 0.2911\n",
            "Epoch 14/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9701 - loss: 0.2003 - val_accuracy: 0.9482 - val_loss: 0.2463\n",
            "Epoch 15/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9705 - loss: 0.1676 - val_accuracy: 0.9422 - val_loss: 0.2733\n",
            "Epoch 1/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 2s/step - accuracy: 0.9144 - loss: 0.3189 - val_accuracy: 0.7562 - val_loss: 0.5409\n",
            "Epoch 2/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9583 - loss: 0.1544 - val_accuracy: 0.8612 - val_loss: 0.3726\n",
            "Epoch 3/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9646 - loss: 0.1341 - val_accuracy: 0.8995 - val_loss: 0.2910\n",
            "Epoch 4/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9682 - loss: 0.1236 - val_accuracy: 0.9231 - val_loss: 0.2762\n",
            "Epoch 5/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9698 - loss: 0.1121 - val_accuracy: 0.9460 - val_loss: 0.2029\n",
            "Epoch 6/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9716 - loss: 0.1043 - val_accuracy: 0.9432 - val_loss: 0.1951\n",
            "Epoch 7/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9736 - loss: 0.0959 - val_accuracy: 0.9364 - val_loss: 0.2130\n",
            "Epoch 8/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9722 - loss: 0.0977 - val_accuracy: 0.9482 - val_loss: 0.2031\n",
            "Epoch 9/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9779 - loss: 0.0869 - val_accuracy: 0.9441 - val_loss: 0.1888\n",
            "Epoch 10/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9781 - loss: 0.0858 - val_accuracy: 0.9452 - val_loss: 0.2125\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6s/step - accuracy: 0.9304 - loss: 0.2438\n",
            "Final Test Accuracy: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resnet50 model training for the augmented dataset of Channel shift method"
      ],
      "metadata": {
        "id": "Cty2lvY90nHW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i1NO1-NrwfT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, Normalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Disable Mixed Precision (use float32 everywhere)\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "set_global_policy('float32')\n",
        "\n",
        "# Copy dataset from Google Drive to Colab RAM (Faster than streaming)\n",
        "os.makedirs('/content/dataset', exist_ok=True)\n",
        "if not os.path.exists('/content/dataset/train'):\n",
        "    print(\"Copying dataset to Colab RAM...\")\n",
        "    shutil.copytree('/content/drive/MyDrive/train', '/content/dataset/train')\n",
        "    shutil.copytree('/content/drive/MyDrive/val', '/content/dataset/val')\n",
        "    shutil.copytree('/content/drive/MyDrive/test', '/content/dataset/test')\n",
        "\n",
        "# Define dataset directories\n",
        "train_dir = '/content/dataset/train'\n",
        "val_dir = '/content/dataset/val'\n",
        "test_dir = '/content/dataset/test'\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 128\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Load dataset using `tf.data`\n",
        "raw_train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "class_names = raw_train_dataset.class_names\n",
        "\n",
        "# Define Data Augmentation Layer\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "    tf.keras.layers.RandomBrightness(0.1),\n",
        "])\n",
        "\n",
        "# Define normalization layer\n",
        "normalization_layer = Normalization()\n",
        "\n",
        "# Function to convert images to float32\n",
        "def cast_to_float32(image, label):\n",
        "    return tf.cast(image, tf.float32), label\n",
        "\n",
        "# Function to apply augmentation + normalization\n",
        "def preprocess_image(image, label):\n",
        "    image = data_augmentation(image)\n",
        "    image = normalization_layer(image)\n",
        "    return image, label\n",
        "\n",
        "# Convert dataset images to float32 before applying normalization\n",
        "normalized_train_dataset = raw_train_dataset.map(cast_to_float32)\n",
        "\n",
        "# Adapt Normalization Layer using training data (only on images)\n",
        "normalization_layer.adapt(normalized_train_dataset.map(lambda x, y: x))\n",
        "\n",
        "# Apply augmentation & normalization to dataset\n",
        "train_dataset = normalized_train_dataset.map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ").map(cast_to_float32).map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ").map(cast_to_float32).map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Load Pre-trained ResNet50 with Correct Input Shape\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add Custom Classification Layers\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu', kernel_regularizer=l2(0.005))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.6)(x)  # Dropout Increased\n",
        "x = Dense(256, activation='relu', kernel_regularizer=l2(0.005))(x)\n",
        "x = Dropout(0.6)(x)\n",
        "\n",
        "# Use `len(class_names)`\n",
        "outputs = Dense(len(class_names), activation='softmax')(x)\n",
        "\n",
        "# Define the Model\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# Freeze the First 120 Layers (ResNet50 has ~175 layers)\n",
        "for layer in base_model.layers[:120]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile Model with Optimized Learning Rate\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=5e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Add Early Stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=15,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        "\n",
        "\n",
        "# Unfreeze More Layers & Fine-tune\n",
        "for layer in base_model.layers[80:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# **Implement Learning Rate Decay**\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=5e-5,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.95\n",
        ")\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# Compile Again with Learning Rate Decay\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "history_fine = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate on Test Set\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "# Save Model\n",
        "model.save('/content/drive/MyDrive/saved_models/resnet_model3.keras')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f57ae706-3d8c-4bdf-fca5-4e097f529bd2",
        "id": "GQL_8EiLLaJd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying dataset to Colab RAM...\n",
            "Found 16460 files belonging to 2 classes.\n",
            "Found 4668 files belonging to 2 classes.\n",
            "Found 615 files belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 2s/step - accuracy: 0.7327 - loss: 7.0522 - val_accuracy: 0.6433 - val_loss: 4.3755\n",
            "Epoch 2/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.8534 - loss: 2.6641 - val_accuracy: 0.7644 - val_loss: 1.9237\n",
            "Epoch 3/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.8870 - loss: 1.3659 - val_accuracy: 0.8203 - val_loss: 1.1061\n",
            "Epoch 4/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9051 - loss: 0.8460 - val_accuracy: 0.8912 - val_loss: 0.6832\n",
            "Epoch 5/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9147 - loss: 0.6133 - val_accuracy: 0.8920 - val_loss: 0.5495\n",
            "Epoch 6/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9315 - loss: 0.4538 - val_accuracy: 0.8888 - val_loss: 0.4911\n",
            "Epoch 7/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9352 - loss: 0.3751 - val_accuracy: 0.8751 - val_loss: 0.5073\n",
            "Epoch 8/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9381 - loss: 0.3203 - val_accuracy: 0.8871 - val_loss: 0.3961\n",
            "Epoch 9/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9436 - loss: 0.2765 - val_accuracy: 0.8929 - val_loss: 0.4016\n",
            "Epoch 10/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9493 - loss: 0.2344 - val_accuracy: 0.8942 - val_loss: 0.3785\n",
            "Epoch 11/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9576 - loss: 0.1966 - val_accuracy: 0.8884 - val_loss: 0.3827\n",
            "Epoch 12/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9561 - loss: 0.1977 - val_accuracy: 0.9152 - val_loss: 0.2984\n",
            "Epoch 13/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9629 - loss: 0.1719 - val_accuracy: 0.9049 - val_loss: 0.3424\n",
            "Epoch 14/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9593 - loss: 0.1878 - val_accuracy: 0.9032 - val_loss: 0.3200\n",
            "Epoch 15/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9632 - loss: 0.1601 - val_accuracy: 0.8730 - val_loss: 0.4083\n",
            "Epoch 1/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 2s/step - accuracy: 0.9164 - loss: 0.2929 - val_accuracy: 0.8802 - val_loss: 0.3304\n",
            "Epoch 2/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9655 - loss: 0.1363 - val_accuracy: 0.9064 - val_loss: 0.2796\n",
            "Epoch 3/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9727 - loss: 0.1094 - val_accuracy: 0.9152 - val_loss: 0.2719\n",
            "Epoch 4/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9703 - loss: 0.1024 - val_accuracy: 0.9105 - val_loss: 0.2805\n",
            "Epoch 5/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9739 - loss: 0.1021 - val_accuracy: 0.9167 - val_loss: 0.2687\n",
            "Epoch 6/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9782 - loss: 0.0867 - val_accuracy: 0.9175 - val_loss: 0.2777\n",
            "Epoch 7/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9775 - loss: 0.0839 - val_accuracy: 0.9252 - val_loss: 0.2565\n",
            "Epoch 8/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9795 - loss: 0.0810 - val_accuracy: 0.9261 - val_loss: 0.2507\n",
            "Epoch 9/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9851 - loss: 0.0663 - val_accuracy: 0.9210 - val_loss: 0.2943\n",
            "Epoch 10/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 2s/step - accuracy: 0.9816 - loss: 0.0688 - val_accuracy: 0.9165 - val_loss: 0.3039\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.9301 - loss: 0.2384\n",
            "Final Test Accuracy: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resnet50 model training for the augmented dataset of Brightnes shift method"
      ],
      "metadata": {
        "id": "MkP_ZbX2wj1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, Normalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Disable Mixed Precision (use float32 everywhere)\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "set_global_policy('float32')\n",
        "\n",
        "# Copy dataset from Google Drive to Colab RAM (Faster than streaming)\n",
        "os.makedirs('/content/dataset', exist_ok=True)\n",
        "if not os.path.exists('/content/dataset/train'):\n",
        "    print(\"Copying dataset to Colab RAM...\")\n",
        "    shutil.copytree('/content/drive/MyDrive/train', '/content/dataset/train')\n",
        "    shutil.copytree('/content/drive/MyDrive/val', '/content/dataset/val')\n",
        "    shutil.copytree('/content/drive/MyDrive/test', '/content/dataset/test')\n",
        "\n",
        "# Define dataset directories\n",
        "train_dir = '/content/dataset/train'\n",
        "val_dir = '/content/dataset/val'\n",
        "test_dir = '/content/dataset/test'\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 128\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Load dataset using `tf.data`\n",
        "raw_train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "class_names = raw_train_dataset.class_names\n",
        "\n",
        "# Define Data Augmentation Layer\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "    tf.keras.layers.RandomBrightness(0.1),\n",
        "])\n",
        "\n",
        "# Define normalization layer\n",
        "normalization_layer = Normalization()\n",
        "\n",
        "# Function to convert images to float32\n",
        "def cast_to_float32(image, label):\n",
        "    return tf.cast(image, tf.float32), label\n",
        "\n",
        "# Function to apply augmentation + normalization\n",
        "def preprocess_image(image, label):\n",
        "    image = data_augmentation(image)\n",
        "    image = normalization_layer(image)\n",
        "    return image, label\n",
        "\n",
        "# Convert dataset images to float32 before applying normalization\n",
        "normalized_train_dataset = raw_train_dataset.map(cast_to_float32)\n",
        "\n",
        "# Adapt Normalization Layer using training data (only on images)\n",
        "normalization_layer.adapt(normalized_train_dataset.map(lambda x, y: x))\n",
        "\n",
        "# Apply augmentation & normalization to dataset\n",
        "train_dataset = normalized_train_dataset.map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ").map(cast_to_float32).map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ").map(cast_to_float32).map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Load Pre-trained ResNet50 with Correct Input Shape\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add Custom Classification Layers\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu', kernel_regularizer=l2(0.005))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.6)(x)\n",
        "x = Dense(256, activation='relu', kernel_regularizer=l2(0.005))(x)\n",
        "x = Dropout(0.6)(x)\n",
        "\n",
        "# Use `len(class_names)`\n",
        "outputs = Dense(len(class_names), activation='softmax')(x)\n",
        "\n",
        "# Define the Model\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# Freeze the First 120 Layers (ResNet50 has ~175 layers)\n",
        "for layer in base_model.layers[:120]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile Model with Optimized Learning Rate\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=5e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Add Early Stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=15,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Unfreeze More Layers & Fine-tune\n",
        "for layer in base_model.layers[80:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# **Implement Learning Rate Decay**\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=5e-5,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.95\n",
        ")\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# Compile Again with Learning Rate Decay\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "history_fine = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate on Test Set\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "# Save Model\n",
        "model.save('/content/drive/MyDrive/saved_models/resnet_model4.keras')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui5tZyS0ggA-",
        "outputId": "d846734a-3fc4-4605-f1b4-ad806f4231f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16460 files belonging to 2 classes.\n",
            "Found 4668 files belonging to 2 classes.\n",
            "Found 615 files belonging to 2 classes.\n",
            "Epoch 1/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 2s/step - accuracy: 0.6664 - loss: 7.2657 - val_accuracy: 0.6140 - val_loss: 7.0779\n",
            "Epoch 2/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.7601 - loss: 3.0931 - val_accuracy: 0.7406 - val_loss: 1.9953\n",
            "Epoch 3/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8253 - loss: 1.6146 - val_accuracy: 0.6954 - val_loss: 1.3543\n",
            "Epoch 4/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8379 - loss: 1.0877 - val_accuracy: 0.7943 - val_loss: 0.9703\n",
            "Epoch 5/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8692 - loss: 0.8092 - val_accuracy: 0.8674 - val_loss: 0.7193\n",
            "Epoch 6/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8707 - loss: 0.6628 - val_accuracy: 0.8824 - val_loss: 0.5882\n",
            "Epoch 7/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8878 - loss: 0.5357 - val_accuracy: 0.8858 - val_loss: 0.5295\n",
            "Epoch 8/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8894 - loss: 0.5051 - val_accuracy: 0.8807 - val_loss: 0.4835\n",
            "Epoch 9/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8972 - loss: 0.4475 - val_accuracy: 0.8927 - val_loss: 0.4304\n",
            "Epoch 10/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9096 - loss: 0.3719 - val_accuracy: 0.8282 - val_loss: 0.5398\n",
            "Epoch 11/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9138 - loss: 0.3210 - val_accuracy: 0.8888 - val_loss: 0.3662\n",
            "Epoch 12/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9150 - loss: 0.2970 - val_accuracy: 0.8740 - val_loss: 0.3945\n",
            "Epoch 13/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9169 - loss: 0.2812 - val_accuracy: 0.8805 - val_loss: 0.3655\n",
            "Epoch 14/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9242 - loss: 0.2681 - val_accuracy: 0.8832 - val_loss: 0.3332\n",
            "Epoch 15/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 2s/step - accuracy: 0.9295 - loss: 0.2387 - val_accuracy: 0.8775 - val_loss: 0.3766\n",
            "Epoch 1/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 2s/step - accuracy: 0.8714 - loss: 0.3828 - val_accuracy: 0.6913 - val_loss: 0.6489\n",
            "Epoch 2/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9298 - loss: 0.2188 - val_accuracy: 0.8768 - val_loss: 0.3262\n",
            "Epoch 3/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9380 - loss: 0.1898 - val_accuracy: 0.9057 - val_loss: 0.2680\n",
            "Epoch 4/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9444 - loss: 0.1692 - val_accuracy: 0.8997 - val_loss: 0.2955\n",
            "Epoch 5/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9528 - loss: 0.1528 - val_accuracy: 0.9107 - val_loss: 0.2702\n",
            "Epoch 6/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9554 - loss: 0.1451 - val_accuracy: 0.9171 - val_loss: 0.2466\n",
            "Epoch 7/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9611 - loss: 0.1297 - val_accuracy: 0.9162 - val_loss: 0.2565\n",
            "Epoch 8/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9621 - loss: 0.1210 - val_accuracy: 0.9145 - val_loss: 0.2572\n",
            "Epoch 9/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9645 - loss: 0.1183 - val_accuracy: 0.9177 - val_loss: 0.2339\n",
            "Epoch 10/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9699 - loss: 0.1066 - val_accuracy: 0.9173 - val_loss: 0.2448\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.8901 - loss: 0.2949\n",
            "Final Test Accuracy: 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m8-UahZhw7Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientnetB0 model training for the augmented dataset of Brightnes shift method"
      ],
      "metadata": {
        "id": "TN0lPBa0xFUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, Normalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Disable Mixed Precision (use float32 everywhere)\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "set_global_policy('float32')\n",
        "\n",
        "# Copy dataset from Google Drive to Colab RAM (Faster than streaming)\n",
        "os.makedirs('/content/dataset', exist_ok=True)\n",
        "if not os.path.exists('/content/dataset/train'):\n",
        "    print(\"Copying dataset to Colab RAM...\")\n",
        "    shutil.copytree('/content/drive/MyDrive/train', '/content/dataset/train')\n",
        "    shutil.copytree('/content/drive/MyDrive/val', '/content/dataset/val')\n",
        "    shutil.copytree('/content/drive/MyDrive/test', '/content/dataset/test')\n",
        "\n",
        "# Define dataset directories\n",
        "train_dir = '/content/dataset/train'\n",
        "val_dir = '/content/dataset/val'\n",
        "test_dir = '/content/dataset/test'\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 128\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Load dataset using `tf.data`\n",
        "raw_train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "class_names = raw_train_dataset.class_names\n",
        "\n",
        "# Define Data Augmentation Layer\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "    tf.keras.layers.RandomBrightness(0.1),\n",
        "])\n",
        "\n",
        "# Define normalization layer\n",
        "normalization_layer = Normalization()\n",
        "\n",
        "# Function to convert images to float32\n",
        "def cast_to_float32(image, label):\n",
        "    return tf.cast(image, tf.float32), label\n",
        "\n",
        "# Function to apply augmentation + normalization\n",
        "def preprocess_image(image, label):\n",
        "    image = data_augmentation(image)\n",
        "    image = normalization_layer(image)\n",
        "    return image, label\n",
        "\n",
        "# Convert dataset images to float32 before applying normalization\n",
        "normalized_train_dataset = raw_train_dataset.map(cast_to_float32)\n",
        "\n",
        "# Adapt Normalization Layer using training data (only on images)\n",
        "normalization_layer.adapt(normalized_train_dataset.map(lambda x, y: x))\n",
        "\n",
        "# Apply augmentation & normalization to dataset\n",
        "train_dataset = normalized_train_dataset.map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ").map(cast_to_float32).map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ").map(cast_to_float32).map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Load Pre-trained EfficientNetB0 with Correct Input Shape\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add Custom Classification Layers\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu', kernel_regularizer=l2(0.005))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.6)(x)  # Dropout Increased\n",
        "x = Dense(256, activation='relu', kernel_regularizer=l2(0.005))(x)\n",
        "x = Dropout(0.6)(x)\n",
        "\n",
        "# Use `len(class_names)`\n",
        "outputs = Dense(len(class_names), activation='softmax')(x)\n",
        "\n",
        "# Define the Model\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# Freeze the First 120 Layers (EfficientNetB0 has ~236 layers)\n",
        "for layer in base_model.layers[:120]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile Model with Optimized Learning Rate\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=5e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Add Early Stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=15,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Unfreeze More Layers & Fine-tune\n",
        "for layer in base_model.layers[80:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# **Implement Learning Rate Decay**\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=5e-5,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.95\n",
        ")\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# Compile Again with Learning Rate Decay\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "history_fine = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate on Test Set\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "# Save Model\n",
        "model.save('/content/drive/MyDrive/saved_models/efficientnet_model4.keras')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9dxH6zEAQXd",
        "outputId": "53c3b411-4043-448b-baca-fc24f6b59c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying dataset to Colab RAM...\n",
            "Found 16460 files belonging to 2 classes.\n",
            "Found 4668 files belonging to 2 classes.\n",
            "Found 615 files belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m357s\u001b[0m 2s/step - accuracy: 0.6710 - loss: 7.9658 - val_accuracy: 0.5570 - val_loss: 5.3012\n",
            "Epoch 2/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8095 - loss: 4.2413 - val_accuracy: 0.7817 - val_loss: 2.6021\n",
            "Epoch 3/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8591 - loss: 2.1416 - val_accuracy: 0.8380 - val_loss: 1.5438\n",
            "Epoch 4/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8782 - loss: 1.3423 - val_accuracy: 0.8948 - val_loss: 1.0308\n",
            "Epoch 5/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8995 - loss: 0.9359 - val_accuracy: 0.8734 - val_loss: 0.8402\n",
            "Epoch 6/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9107 - loss: 0.7515 - val_accuracy: 0.9180 - val_loss: 0.6348\n",
            "Epoch 7/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9249 - loss: 0.5887 - val_accuracy: 0.9124 - val_loss: 0.5557\n",
            "Epoch 8/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9305 - loss: 0.5014 - val_accuracy: 0.9265 - val_loss: 0.4470\n",
            "Epoch 9/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9360 - loss: 0.4079 - val_accuracy: 0.9188 - val_loss: 0.4395\n",
            "Epoch 10/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9394 - loss: 0.3460 - val_accuracy: 0.9184 - val_loss: 0.3871\n",
            "Epoch 11/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9460 - loss: 0.3066 - val_accuracy: 0.9265 - val_loss: 0.3598\n",
            "Epoch 12/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9494 - loss: 0.2727 - val_accuracy: 0.9244 - val_loss: 0.3233\n",
            "Epoch 13/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9527 - loss: 0.2586 - val_accuracy: 0.9252 - val_loss: 0.3058\n",
            "Epoch 14/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 2s/step - accuracy: 0.9528 - loss: 0.2330 - val_accuracy: 0.9293 - val_loss: 0.3153\n",
            "Epoch 15/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 2s/step - accuracy: 0.9545 - loss: 0.2270 - val_accuracy: 0.9246 - val_loss: 0.3435\n",
            "Epoch 1/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 2s/step - accuracy: 0.8745 - loss: 0.4275 - val_accuracy: 0.8044 - val_loss: 0.5077\n",
            "Epoch 2/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9354 - loss: 0.2331 - val_accuracy: 0.8852 - val_loss: 0.3540\n",
            "Epoch 3/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9490 - loss: 0.1966 - val_accuracy: 0.9214 - val_loss: 0.2633\n",
            "Epoch 4/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9536 - loss: 0.1803 - val_accuracy: 0.9344 - val_loss: 0.2277\n",
            "Epoch 5/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9566 - loss: 0.1665 - val_accuracy: 0.9385 - val_loss: 0.2145\n",
            "Epoch 6/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9606 - loss: 0.1521 - val_accuracy: 0.9402 - val_loss: 0.2104\n",
            "Epoch 7/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9644 - loss: 0.1430 - val_accuracy: 0.9439 - val_loss: 0.1982\n",
            "Epoch 8/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9622 - loss: 0.1483 - val_accuracy: 0.9434 - val_loss: 0.1988\n",
            "Epoch 9/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9659 - loss: 0.1319 - val_accuracy: 0.9454 - val_loss: 0.1948\n",
            "Epoch 10/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9661 - loss: 0.1298 - val_accuracy: 0.9398 - val_loss: 0.1972\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6s/step - accuracy: 0.9267 - loss: 0.2250\n",
            "Final Test Accuracy: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BqWVCKCThNaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resnet50 model training for the augmented dataset of both Brightnes and random channel"
      ],
      "metadata": {
        "id": "27zjlMkqhOKK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3siQMfqRh_xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b2228d4-2a7d-49d4-bafc-95e8e0965193",
        "id": "GYeJY_z8iEJc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying dataset to Colab RAM...\n",
            "Found 16460 files belonging to 2 classes.\n",
            "Found 4668 files belonging to 2 classes.\n",
            "Found 615 files belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 2s/step - accuracy: 0.6666 - loss: 7.2984 - val_accuracy: 0.6144 - val_loss: 4.4118\n",
            "Epoch 2/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.7850 - loss: 3.0645 - val_accuracy: 0.7791 - val_loss: 2.0329\n",
            "Epoch 3/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.8236 - loss: 1.6971 - val_accuracy: 0.8085 - val_loss: 1.2499\n",
            "Epoch 4/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8479 - loss: 1.1219 - val_accuracy: 0.8436 - val_loss: 0.9282\n",
            "Epoch 5/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8652 - loss: 0.8314 - val_accuracy: 0.8179 - val_loss: 0.8256\n",
            "Epoch 6/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.8793 - loss: 0.6910 - val_accuracy: 0.8380 - val_loss: 0.6757\n",
            "Epoch 7/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8914 - loss: 0.5685 - val_accuracy: 0.8790 - val_loss: 0.5957\n",
            "Epoch 8/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8961 - loss: 0.5178 - val_accuracy: 0.8207 - val_loss: 0.6742\n",
            "Epoch 9/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9028 - loss: 0.4285 - val_accuracy: 0.8760 - val_loss: 0.4949\n",
            "Epoch 10/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9094 - loss: 0.3847 - val_accuracy: 0.8460 - val_loss: 0.4826\n",
            "Epoch 11/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9170 - loss: 0.3260 - val_accuracy: 0.8882 - val_loss: 0.3856\n",
            "Epoch 12/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9196 - loss: 0.3045 - val_accuracy: 0.8841 - val_loss: 0.3634\n",
            "Epoch 13/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9242 - loss: 0.2799 - val_accuracy: 0.8877 - val_loss: 0.3768\n",
            "Epoch 14/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9217 - loss: 0.2686 - val_accuracy: 0.8548 - val_loss: 0.4804\n",
            "Epoch 15/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9284 - loss: 0.2544 - val_accuracy: 0.8937 - val_loss: 0.3467\n",
            "Epoch 1/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 2s/step - accuracy: 0.8703 - loss: 0.3912 - val_accuracy: 0.7395 - val_loss: 0.6600\n",
            "Epoch 2/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9286 - loss: 0.2246 - val_accuracy: 0.8702 - val_loss: 0.3439\n",
            "Epoch 3/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9409 - loss: 0.1869 - val_accuracy: 0.9081 - val_loss: 0.2652\n",
            "Epoch 4/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9471 - loss: 0.1676 - val_accuracy: 0.9049 - val_loss: 0.2658\n",
            "Epoch 5/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 2s/step - accuracy: 0.9519 - loss: 0.1516 - val_accuracy: 0.9072 - val_loss: 0.2611\n",
            "Epoch 6/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9529 - loss: 0.1449 - val_accuracy: 0.9111 - val_loss: 0.2506\n",
            "Epoch 7/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9548 - loss: 0.1355 - val_accuracy: 0.9152 - val_loss: 0.2539\n",
            "Epoch 8/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9593 - loss: 0.1278 - val_accuracy: 0.9235 - val_loss: 0.2295\n",
            "Epoch 9/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9614 - loss: 0.1208 - val_accuracy: 0.9216 - val_loss: 0.2419\n",
            "Epoch 10/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9676 - loss: 0.1081 - val_accuracy: 0.9173 - val_loss: 0.2618\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2s/step - accuracy: 0.9043 - loss: 0.2644\n",
            "Final Test Accuracy: 0.90\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, Normalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Disable Mixed Precision (use float32 everywhere)\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "set_global_policy('float32')\n",
        "\n",
        "# Copy dataset from Google Drive to Colab RAM (Faster than streaming)\n",
        "os.makedirs('/content/dataset', exist_ok=True)\n",
        "if not os.path.exists('/content/dataset/train'):\n",
        "    print(\"Copying dataset to Colab RAM...\")\n",
        "    shutil.copytree('/content/drive/MyDrive/train', '/content/dataset/train')\n",
        "    shutil.copytree('/content/drive/MyDrive/val', '/content/dataset/val')\n",
        "    shutil.copytree('/content/drive/MyDrive/test', '/content/dataset/test')\n",
        "\n",
        "# Define dataset directories\n",
        "train_dir = '/content/dataset/train'\n",
        "val_dir = '/content/dataset/val'\n",
        "test_dir = '/content/dataset/test'\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 128\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Load dataset using `tf.data`\n",
        "raw_train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "class_names = raw_train_dataset.class_names\n",
        "\n",
        "# Define Data Augmentation Layer\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "    tf.keras.layers.RandomBrightness(0.1),\n",
        "])\n",
        "\n",
        "# Define normalization layer\n",
        "normalization_layer = Normalization()\n",
        "\n",
        "# Function to convert images to float32\n",
        "def cast_to_float32(image, label):\n",
        "    return tf.cast(image, tf.float32), label\n",
        "\n",
        "# Function to apply augmentation + normalization\n",
        "def preprocess_image(image, label):\n",
        "    image = data_augmentation(image)\n",
        "    image = normalization_layer(image)\n",
        "    return image, label\n",
        "\n",
        "# Convert dataset images to float32 before applying normalization\n",
        "normalized_train_dataset = raw_train_dataset.map(cast_to_float32)\n",
        "\n",
        "# Adapt Normalization Layer using training data (only on images)\n",
        "normalization_layer.adapt(normalized_train_dataset.map(lambda x, y: x))\n",
        "\n",
        "# Apply augmentation & normalization to dataset\n",
        "train_dataset = normalized_train_dataset.map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ").map(cast_to_float32).map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ").map(cast_to_float32).map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Load Pre-trained ResNet50 with Correct Input Shape\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add Custom Classification Layers\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu', kernel_regularizer=l2(0.005))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.6)(x)\n",
        "x = Dense(256, activation='relu', kernel_regularizer=l2(0.005))(x)\n",
        "x = Dropout(0.6)(x)\n",
        "\n",
        "# Use `len(class_names)`\n",
        "outputs = Dense(len(class_names), activation='softmax')(x)\n",
        "\n",
        "# Define the Model\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# Freeze the First 120 Layers (ResNet50 has ~175 layers)\n",
        "for layer in base_model.layers[:120]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile Model with Optimized Learning Rate\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=5e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Add Early Stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=15,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Unfreeze More Layers & Fine-tune\n",
        "for layer in base_model.layers[80:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# **Implement Learning Rate Decay**\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=5e-5,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.95\n",
        ")\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# Compile Again with Learning Rate Decay\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "history_fine = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate on Test Set\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "# Save Model\n",
        "model.save('/content/drive/MyDrive/saved_models/resnet_model5.keras')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientnetB0 model training for the augmented dataset of both Brightnes and random channel"
      ],
      "metadata": {
        "id": "meYit059iGoo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bfQTl5n3iUHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8dcf2c2-a394-4833-dcd9-79f1a306232e",
        "id": "xjWH7PzevP5W"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 16460 files belonging to 2 classes.\n",
            "Found 4668 files belonging to 2 classes.\n",
            "Found 615 files belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m354s\u001b[0m 2s/step - accuracy: 0.6548 - loss: 8.0239 - val_accuracy: 0.5281 - val_loss: 5.6476\n",
            "Epoch 2/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.7944 - loss: 4.2322 - val_accuracy: 0.6050 - val_loss: 3.1446\n",
            "Epoch 3/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8402 - loss: 2.2439 - val_accuracy: 0.8558 - val_loss: 1.5711\n",
            "Epoch 4/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8650 - loss: 1.4007 - val_accuracy: 0.9002 - val_loss: 1.0208\n",
            "Epoch 5/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8917 - loss: 0.9689 - val_accuracy: 0.8342 - val_loss: 0.9583\n",
            "Epoch 6/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.8999 - loss: 0.7577 - val_accuracy: 0.9147 - val_loss: 0.6378\n",
            "Epoch 7/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9086 - loss: 0.6185 - val_accuracy: 0.9060 - val_loss: 0.5502\n",
            "Epoch 8/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9228 - loss: 0.5091 - val_accuracy: 0.8488 - val_loss: 0.5973\n",
            "Epoch 9/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9236 - loss: 0.4455 - val_accuracy: 0.9195 - val_loss: 0.4414\n",
            "Epoch 10/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9335 - loss: 0.3961 - val_accuracy: 0.9145 - val_loss: 0.4081\n",
            "Epoch 11/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9418 - loss: 0.3463 - val_accuracy: 0.9216 - val_loss: 0.3463\n",
            "Epoch 12/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9420 - loss: 0.2933 - val_accuracy: 0.9126 - val_loss: 0.3615\n",
            "Epoch 13/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9416 - loss: 0.2818 - val_accuracy: 0.9152 - val_loss: 0.3476\n",
            "Epoch 14/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9473 - loss: 0.2601 - val_accuracy: 0.9141 - val_loss: 0.3118\n",
            "Epoch 15/15\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9481 - loss: 0.2394 - val_accuracy: 0.8713 - val_loss: 0.5268\n",
            "Epoch 1/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 2s/step - accuracy: 0.8862 - loss: 0.3869 - val_accuracy: 0.8498 - val_loss: 0.4386\n",
            "Epoch 2/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9304 - loss: 0.2281 - val_accuracy: 0.8886 - val_loss: 0.3288\n",
            "Epoch 3/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9424 - loss: 0.2006 - val_accuracy: 0.9120 - val_loss: 0.2750\n",
            "Epoch 4/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9494 - loss: 0.1760 - val_accuracy: 0.9314 - val_loss: 0.2396\n",
            "Epoch 5/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9529 - loss: 0.1637 - val_accuracy: 0.9336 - val_loss: 0.2153\n",
            "Epoch 6/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9555 - loss: 0.1580 - val_accuracy: 0.9357 - val_loss: 0.2160\n",
            "Epoch 7/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9606 - loss: 0.1514 - val_accuracy: 0.9407 - val_loss: 0.2158\n",
            "Epoch 8/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9619 - loss: 0.1443 - val_accuracy: 0.9398 - val_loss: 0.2054\n",
            "Epoch 9/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9631 - loss: 0.1347 - val_accuracy: 0.9447 - val_loss: 0.1968\n",
            "Epoch 10/10\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 2s/step - accuracy: 0.9624 - loss: 0.1366 - val_accuracy: 0.9473 - val_loss: 0.1871\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 5s/step - accuracy: 0.9393 - loss: 0.2120\n",
            "Final Test Accuracy: 0.93\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, Normalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Disable Mixed Precision (use float32 everywhere)\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "set_global_policy('float32')\n",
        "\n",
        "# Copy dataset from Google Drive to Colab RAM (Faster than streaming)\n",
        "os.makedirs('/content/dataset', exist_ok=True)\n",
        "if not os.path.exists('/content/dataset/train'):\n",
        "    print(\"Copying dataset to Colab RAM...\")\n",
        "    shutil.copytree('/content/drive/MyDrive/train', '/content/dataset/train')\n",
        "    shutil.copytree('/content/drive/MyDrive/val', '/content/dataset/val')\n",
        "    shutil.copytree('/content/drive/MyDrive/test', '/content/dataset/test')\n",
        "\n",
        "# Define dataset directories\n",
        "train_dir = '/content/dataset/train'\n",
        "val_dir = '/content/dataset/val'\n",
        "test_dir = '/content/dataset/test'\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 128\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Load dataset using `tf.data`\n",
        "raw_train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "class_names = raw_train_dataset.class_names\n",
        "\n",
        "# Define Data Augmentation Layer\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "    tf.keras.layers.RandomBrightness(0.1),\n",
        "])\n",
        "\n",
        "# Define normalization layer\n",
        "normalization_layer = Normalization()\n",
        "\n",
        "# Function to convert images to float32\n",
        "def cast_to_float32(image, label):\n",
        "    return tf.cast(image, tf.float32), label\n",
        "\n",
        "# Function to apply augmentation + normalization\n",
        "def preprocess_image(image, label):\n",
        "    image = data_augmentation(image)\n",
        "    image = normalization_layer(image)\n",
        "    return image, label\n",
        "\n",
        "# Convert dataset images to float32 before applying normalization\n",
        "normalized_train_dataset = raw_train_dataset.map(cast_to_float32)\n",
        "\n",
        "# Adapt Normalization Layer using training data (only on images)\n",
        "normalization_layer.adapt(normalized_train_dataset.map(lambda x, y: x))\n",
        "\n",
        "# Apply augmentation & normalization to dataset\n",
        "train_dataset = normalized_train_dataset.map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ").map(cast_to_float32).map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE\n",
        ").map(cast_to_float32).map(preprocess_image).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Load Pre-trained EfficientNetB0 with Correct Input Shape\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add Custom Classification Layers\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu', kernel_regularizer=l2(0.005))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.6)(x)  # Dropout Increased\n",
        "x = Dense(256, activation='relu', kernel_regularizer=l2(0.005))(x)\n",
        "x = Dropout(0.6)(x)\n",
        "\n",
        "# Use `len(class_names)`\n",
        "outputs = Dense(len(class_names), activation='softmax')(x)\n",
        "\n",
        "# Define the Model\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "# Freeze the First 120 Layers (EfficientNetB0 has ~236 layers)\n",
        "for layer in base_model.layers[:120]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile Model with Optimized Learning Rate\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=5e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Add Early Stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=15,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Unfreeze More Layers & Fine-tune\n",
        "for layer in base_model.layers[80:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# **Implement Learning Rate Decay**\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=5e-5,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.95\n",
        ")\n",
        "optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "# Compile Again with Learning Rate Decay\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "history_fine = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate on Test Set\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.2f}\")\n",
        "\n",
        "# Save Model\n",
        "model.save('/content/drive/MyDrive/saved_models/efficientnet_model5.keras')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}