Bias in medical imaging datasets, particularly in histology, can significantly affect the performance and generalization of machine learning models. Variations in staining intensity, image acquisition protocols, and under-representation of certain classes introduce artifacts that may lead to model misclassifications. These biases often cause the model to rely on non-diagnostic features, such as darker or brighter regions, rather than relevant pathological information. This study explores the relationship between histology image features, specifically average darkness, and model misclassifications using Explainable AI (XAI) techniques.

The methodology involves extracting tiles from whole slide images (WSIs) and calculating average darkness and other relevant image features for each tile. Statistical analysis is then performed to investigate differences in these features across correctly classified and misclassified tiles. A classification model, such as ResNet or EfficientNet, is employed to predict tile labels, and XAI techniques like Grad-CAM and saliency maps are used to interpret the model's decision-making process. The insights from XAI are combined with statistical findings to determine if image darkness or other features contribute to biases in model predictions.

This research aims to identify and address dataset and model biases, ultimately improving the robustness and accuracy of deep learning models in histopathology. By understanding the interplay between image features and misclassifications, this study provides actionable insights for preprocessing, data augmentation, and model training strategies in medical image analysis.
